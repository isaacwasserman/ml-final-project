{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for `all'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "!make\n",
    "\n",
    "import align\n",
    "import argparse\n",
    "import codecs\n",
    "import os, sys\n",
    "from random import random, choice\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stem_length = 10\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as inp:\n",
    "        lines = inp.readlines()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    tags = []\n",
    "    for l in lines:\n",
    "        l = l.strip().split('\\t')\n",
    "        if l:\n",
    "            inputs.append(list(l[0].strip()))\n",
    "            outputs.append(list(l[1].strip()))\n",
    "            tags.append(re.split('\\W+', l[2].strip()))\n",
    "    return inputs, outputs, tags\n",
    "\n",
    "def find_good_range(a,b):\n",
    "\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tif sum(mask) == 0:\n",
    "\t\t# Some times the alignment is off-by-one\n",
    "\t\tb = [' '] + b\n",
    "\t\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tranges = []\n",
    "\tprev = False\n",
    "\tfor i,k in enumerate(mask):\n",
    "\t\tif k and prev:\n",
    "\t\t\tprev = True\n",
    "\t\telif k and not prev:\n",
    "\t\t\tstart = i\n",
    "\t\t\tprev = True\n",
    "\t\telif prev and not k:\n",
    "\t\t\tend = i\n",
    "\t\t\tranges.append((start, end))\n",
    "\t\t\tprev = False\n",
    "\t\telif not prev and not k:\n",
    "\t\t\tprev = False\n",
    "\tif prev:\n",
    "\t\tranges.append((start,i+1))\n",
    "\tranges = [c for c in ranges if c[1]-c[0]>2]\n",
    "\treturn ranges\n",
    "def generate_stem():\n",
    "\treturn \"___\"\n",
    "\n",
    "def get_chars(l):\n",
    "    flat_list = [char for word in l for char in word]\n",
    "    return list(set(flat_list))\n",
    "def best_range(ranges):\n",
    "    longest_length = 0\n",
    "    longest_index = 0\n",
    "    for i,r in enumerate(ranges):\n",
    "        length = r[1] - r[0]\n",
    "        if length > longest_length:\n",
    "            longest_length = length\n",
    "            longest_index = i\n",
    "    return ranges[i]\n",
    "\n",
    "def augment(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "    aligned = align.Aligner(temp).alignedpairs\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    new_tags = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            new_i, new_o = list(i), list(o)\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            new_stem = generate_stem()\n",
    "            new_i[s:e] = new_stem\n",
    "            new_o[s:e] = new_stem\n",
    "            new_i1 = [c for l,c in enumerate(new_i) if (c.strip() or (new_o[l]==' ' and new_i[l] == ' '))]\n",
    "            new_o1 = [c for l,c in enumerate(new_o) if (c.strip() or (new_i[l]==' ' and new_o[l] == ' '))]\n",
    "            new_inputs.append(new_i1)\n",
    "            new_outputs.append(new_o1)\n",
    "            new_tags.append(tags[k])\n",
    "        else:\n",
    "            new_inputs.append([])\n",
    "            new_outputs.append([])\n",
    "            new_tags.append([])\n",
    "    return new_inputs, new_outputs, new_tags\n",
    "\n",
    "def find_stems(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        aligned = align.Aligner(temp).alignedpairs\n",
    "\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stems = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            stem = o[s:e]\n",
    "            stems.append(stem)\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    return stems\n",
    "\n",
    "def get_vocab(strings):\n",
    "    return sorted(list(get_chars(strings)))\n",
    "\n",
    "def enumerate_sequence_characters(sequences, vocab):\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([lut[char] for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def one_hot_encode_sequence(sequences, vocab):\n",
    "    length = len(vocab)\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([[0] * lut[char] + [1] + [0] * ((length - lut[char]) - 1) for char in sequence])\n",
    "    return np.array(new_sequences), {v: k for k, v in lut.items()}\n",
    "\n",
    "def get_stem_data(language, set_type=\"train\", hilo=None, data_dir=\"sigmorphon_data\", pad=True):\n",
    "    if hilo is None:\n",
    "        if f'{data_dir}/{language}-{set_type}-high' in glob.glob(f'{data_dir}/{language}-{set_type}-*'):\n",
    "            hilo = \"high\"\n",
    "        else:\n",
    "            hilo = \"low\"\n",
    "    dpath = f'{data_dir}/{language}-{set_type}-{hilo}'\n",
    "    padded_stems = sequence.pad_sequences(find_stems(dpath), dtype=str, maxlen=max_stem_length, padding=\"post\", truncating=\"post\")\n",
    "    if pad:\n",
    "        return padded_stems\n",
    "    else:\n",
    "        return np.array([np.array(stem) for stem in find_stems(dpath)])\n",
    "\n",
    "def clean_stems(affirmative_stems, negative_stems):\n",
    "    excluded_chars = [\",\",\"'\",\"/\",\"*\",\"-\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    affirmative_vocab = get_vocab(affirmative_stems)\n",
    "    to_be_excluded = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        for char in stem:\n",
    "            if char not in affirmative_vocab or char in excluded_chars:\n",
    "                to_be_excluded.append(i)\n",
    "                break\n",
    "    new_negative_stems = []\n",
    "    new_affirmative_stems = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        if i not in to_be_excluded:\n",
    "            new_negative_stems.append([character.lower() for character in stem])\n",
    "    for i,stem in enumerate(affirmative_stems):\n",
    "        reject = False\n",
    "        for char in stem:\n",
    "            if char in excluded_chars:\n",
    "                reject = True\n",
    "        if not reject:\n",
    "            new_affirmative_stems.append([character.lower() for character in stem])\n",
    "    return new_affirmative_stems, new_negative_stems\n",
    "\n",
    "def create_stem_dataset(reference_language, other_languages):\n",
    "    reference_stems = get_stem_data(reference_language)\n",
    "    other_stems = np.concatenate([get_stem_data(language) for language in other_languages])\n",
    "    cleaned = clean_stems(reference_stems, other_stems)\n",
    "    reference_stems = cleaned[0]\n",
    "    other_stems = cleaned[1]\n",
    "    combined_stems = np.concatenate([reference_stems, other_stems])\n",
    "    combined_vocab = get_vocab(combined_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(combined_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems) + [0] * len(other_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab\n",
    "\n",
    "def create_single_language_stem_dataset(language, hilo=\"high\"):\n",
    "    reference_stems = get_stem_data(language, set_type=\"train\", hilo=hilo)\n",
    "    cleaned = clean_stems(reference_stems, np.array([]))\n",
    "    reference_stems = cleaned[0]\n",
    "    combined_vocab = get_vocab(reference_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(reference_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.001)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab, lut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livonian\n",
      "['0', 'a', 'b', 'd', 'e', 'f', 'g', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'z', 'ä', 'õ', 'ā', 'ē', 'ī', 'ļ', 'ņ', 'ō', 'ŗ', 'š', 'ū', 'ž', 'ǟ', 'ț', 'ȭ', 'ȯ', 'ȱ', 'ḑ']\n",
      "Epoch 1/1500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zc/s2x1jcqx7gd7y6myf0gmt49c0000gn/T/ipykernel_70275/1961213510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"discriminator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdiscriminator_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerator_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gan\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgan_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zc/s2x1jcqx7gd7y6myf0gmt49c0000gn/T/ipykernel_70275/1961213510.py\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(gan, dataset, batch_size, vocab_len, n_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mgan_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mdiscriminator_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2087\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0m\u001b[1;32m   2090\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m                                                     class_weight)\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3313\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3315\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3316\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "languages = [\"livonian\"]\n",
    "\n",
    "for language in languages:\n",
    "    print(language)\n",
    "    X_train, X_test, y_train, y_test, X_vocab, lut = create_single_language_stem_dataset(language, hilo=\"low\")\n",
    "\n",
    "    vocab_len = len(X_vocab)\n",
    "\n",
    "    def distribution_to_sequence(batch_output):\n",
    "        return tf.one_hot(tf.argmax(batch_output, axis=-1), depth = vocab_len)\n",
    "\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        y_true_adjusted = tf.math.subtract(tf.math.multiply(y_true, 2), 1)\n",
    "        y_pred_adjusted = tf.math.subtract(tf.math.multiply(y_pred, 2), 1)\n",
    "        return backend.mean(y_true_adjusted * y_pred_adjusted)\n",
    "\n",
    "    generator = keras.models.Sequential([\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dense(vocab_len, activation=\"softmax\")\n",
    "    ])\n",
    "    discriminator = keras.models.Sequential([\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        # keras.layers.Dropout(0.2),\n",
    "        # keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "    discriminator.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\", run_eagerly=True)\n",
    "    discriminator.trainable = False\n",
    "    gan.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\", run_eagerly=True)\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "    def train_gan(gan, dataset, batch_size, vocab_len, n_epochs=50):\n",
    "        generator, discriminator = gan.layers\n",
    "        discriminator_history = {\"loss\":[],\"accuracy\":[]}\n",
    "        generator_history = {\"generated_sequences\":[]}\n",
    "        gan_history = {\"loss\":[],\"accuracy\":[]}\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "            discriminator_metrics = []\n",
    "            gan_metrics = []\n",
    "            generated_images = None\n",
    "            for X_batch in dataset:\n",
    "                # phase 1 - training the discriminator\n",
    "                noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "                generated_images = generator(noise)\n",
    "                X_fake_and_real = tf.concat([generated_images, tf.cast(X_batch, dtype=\"float32\")], axis=0)\n",
    "                y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "                discriminator.trainable = True\n",
    "                discriminator_metrics = discriminator.train_on_batch(tf.cast(X_fake_and_real, dtype=\"float32\"), y1)\n",
    "                # phase 2 - training the generator\n",
    "                noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "                y2 = tf.constant([[1.]] * batch_size)\n",
    "                discriminator.trainable = False\n",
    "                gan_metrics = gan.train_on_batch(noise, y2)\n",
    "            for i,v in enumerate(discriminator_metrics):\n",
    "                discriminator_history[discriminator.metrics_names[i]].append(v)\n",
    "            for i,v in enumerate(gan_metrics):\n",
    "                gan_history[gan.metrics_names[i]].append(v)\n",
    "            generator_history[\"generated_sequences\"].append(generated_images[0])\n",
    "            history_df = pd.DataFrame({\n",
    "                \"discriminator_loss\":discriminator_history[\"loss\"],\n",
    "                \"discriminator_accuracy\":discriminator_history[\"accuracy\"],\n",
    "                \"gan_loss\":gan_history[\"loss\"],\n",
    "                \"gan_accuracy\":gan_history[\"accuracy\"],\n",
    "                \"generated_sequences\":[\"\".join([lut[np.argmax(one_hot_char)] for one_hot_char in one_hot_sequence]) for one_hot_sequence in generator_history[\"generated_sequences\"]],\n",
    "            })\n",
    "            history_df.to_csv(f'{language}_gan_history.csv')\n",
    "            if epoch % 50 == 0:\n",
    "                gan.save(f'checkpoints/{language}-epoch-{epoch}.h5', save_format=\"h5\")\n",
    "        return {\"discriminator\": discriminator_history, \"generator\": generator_history, \"gan\": gan_history}\n",
    "\n",
    "    history = train_gan(gan, dataset, batch_size, vocab_len, n_epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
