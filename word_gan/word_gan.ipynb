{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import align\n",
    "import argparse\n",
    "import codecs\n",
    "import os, sys\n",
    "from random import random, choice\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from losses_impl import *\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stem_length = 10\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as inp:\n",
    "        lines = inp.readlines()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    tags = []\n",
    "    for l in lines:\n",
    "        l = l.strip().split('\\t')\n",
    "        if l:\n",
    "            inputs.append(list(l[0].strip()))\n",
    "            outputs.append(list(l[1].strip()))\n",
    "            tags.append(re.split('\\W+', l[2].strip()))\n",
    "    return inputs, outputs, tags\n",
    "\n",
    "def find_good_range(a,b):\n",
    "\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tif sum(mask) == 0:\n",
    "\t\t# Some times the alignment is off-by-one\n",
    "\t\tb = [' '] + b\n",
    "\t\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tranges = []\n",
    "\tprev = False\n",
    "\tfor i,k in enumerate(mask):\n",
    "\t\tif k and prev:\n",
    "\t\t\tprev = True\n",
    "\t\telif k and not prev:\n",
    "\t\t\tstart = i\n",
    "\t\t\tprev = True\n",
    "\t\telif prev and not k:\n",
    "\t\t\tend = i\n",
    "\t\t\tranges.append((start, end))\n",
    "\t\t\tprev = False\n",
    "\t\telif not prev and not k:\n",
    "\t\t\tprev = False\n",
    "\tif prev:\n",
    "\t\tranges.append((start,i+1))\n",
    "\tranges = [c for c in ranges if c[1]-c[0]>2]\n",
    "\treturn ranges\n",
    "def generate_stem():\n",
    "\treturn \"___\"\n",
    "\n",
    "def get_chars(l):\n",
    "    flat_list = [char for word in l for char in word]\n",
    "    return list(set(flat_list))\n",
    "def best_range(ranges):\n",
    "    longest_length = 0\n",
    "    longest_index = 0\n",
    "    for i,r in enumerate(ranges):\n",
    "        length = r[1] - r[0]\n",
    "        if length > longest_length:\n",
    "            longest_length = length\n",
    "            longest_index = i\n",
    "    return ranges[i]\n",
    "\n",
    "def augment(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "    aligned = align.Aligner(temp).alignedpairs\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    new_tags = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            new_i, new_o = list(i), list(o)\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            new_stem = generate_stem()\n",
    "            new_i[s:e] = new_stem\n",
    "            new_o[s:e] = new_stem\n",
    "            new_i1 = [c for l,c in enumerate(new_i) if (c.strip() or (new_o[l]==' ' and new_i[l] == ' '))]\n",
    "            new_o1 = [c for l,c in enumerate(new_o) if (c.strip() or (new_i[l]==' ' and new_o[l] == ' '))]\n",
    "            new_inputs.append(new_i1)\n",
    "            new_outputs.append(new_o1)\n",
    "            new_tags.append(tags[k])\n",
    "        else:\n",
    "            new_inputs.append([])\n",
    "            new_outputs.append([])\n",
    "            new_tags.append([])\n",
    "    return new_inputs, new_outputs, new_tags\n",
    "\n",
    "def find_stems(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        aligned = align.Aligner(temp).alignedpairs\n",
    "\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stems = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            stem = o[s:e]\n",
    "            stems.append(stem)\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    return stems\n",
    "\n",
    "def get_vocab(strings):\n",
    "    return sorted(list(get_chars(strings)))\n",
    "\n",
    "def enumerate_sequence_characters(sequences, vocab):\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([lut[char] for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def one_hot_encode_sequence(sequences, vocab):\n",
    "    length = len(vocab)\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([[0] * lut[char] + [1] + [0] * ((length - lut[char]) - 1) for char in sequence])\n",
    "    return np.array(new_sequences), {v: k for k, v in lut.items()}\n",
    "\n",
    "def get_stem_data(language, set_type=\"train\", hilo=None, data_dir=\"sigmorphon_data\", pad=True):\n",
    "    if hilo is None:\n",
    "        if f'{data_dir}/{language}-{set_type}-high' in glob.glob(f'{data_dir}/{language}-{set_type}-*'):\n",
    "            hilo = \"high\"\n",
    "        else:\n",
    "            hilo = \"low\"\n",
    "    dpath = f'{data_dir}/{language}-{set_type}-{hilo}'\n",
    "    padded_stems = sequence.pad_sequences(find_stems(dpath), dtype=str, maxlen=max_stem_length, padding=\"post\", truncating=\"post\")\n",
    "    if pad:\n",
    "        return padded_stems\n",
    "    else:\n",
    "        return np.array([np.array(stem) for stem in find_stems(dpath)])\n",
    "\n",
    "def clean_stems(affirmative_stems, negative_stems):\n",
    "    excluded_chars = [\",\",\"'\",\"/\",\"*\",\"-\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    affirmative_vocab = get_vocab(affirmative_stems)\n",
    "    to_be_excluded = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        for char in stem:\n",
    "            if char not in affirmative_vocab or char in excluded_chars:\n",
    "                to_be_excluded.append(i)\n",
    "                break\n",
    "    new_negative_stems = []\n",
    "    new_affirmative_stems = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        if i not in to_be_excluded:\n",
    "            new_negative_stems.append([character.lower() for character in stem])\n",
    "    for i,stem in enumerate(affirmative_stems):\n",
    "        reject = False\n",
    "        for char in stem:\n",
    "            if char in excluded_chars:\n",
    "                reject = True\n",
    "        if not reject:\n",
    "            new_affirmative_stems.append([character.lower() for character in stem])\n",
    "    return new_affirmative_stems, new_negative_stems\n",
    "\n",
    "def create_stem_dataset(reference_language, other_languages):\n",
    "    reference_stems = get_stem_data(reference_language)\n",
    "    other_stems = np.concatenate([get_stem_data(language) for language in other_languages])\n",
    "    cleaned = clean_stems(reference_stems, other_stems)\n",
    "    reference_stems = cleaned[0]\n",
    "    other_stems = cleaned[1]\n",
    "    combined_stems = np.concatenate([reference_stems, other_stems])\n",
    "    combined_vocab = get_vocab(combined_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(combined_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems) + [0] * len(other_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab\n",
    "\n",
    "def create_single_language_stem_dataset(language, hilo=\"high\"):\n",
    "    reference_stems = get_stem_data(language, set_type=\"train\", hilo=hilo)\n",
    "    cleaned = clean_stems(reference_stems, np.array([]))\n",
    "    reference_stems = cleaned[0]\n",
    "    combined_vocab = get_vocab(reference_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(reference_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.001)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab, lut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '0', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'é', 'ë', 'ö', 'œ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]]]),\n",
       " array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 1, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0],\n",
       "         [1, 0, 0, ..., 0, 0, 0]]]),\n",
       " array([1, 1, 1, ..., 1, 1, 1]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " ['.',\n",
       "  '0',\n",
       "  'a',\n",
       "  'b',\n",
       "  'c',\n",
       "  'd',\n",
       "  'e',\n",
       "  'f',\n",
       "  'g',\n",
       "  'h',\n",
       "  'i',\n",
       "  'j',\n",
       "  'k',\n",
       "  'l',\n",
       "  'm',\n",
       "  'n',\n",
       "  'o',\n",
       "  'p',\n",
       "  'q',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'u',\n",
       "  'v',\n",
       "  'w',\n",
       "  'x',\n",
       "  'y',\n",
       "  'z',\n",
       "  'æ',\n",
       "  'é',\n",
       "  'ë',\n",
       "  'ö',\n",
       "  'œ'],\n",
       " {0: '0',\n",
       "  1: '.',\n",
       "  2: 'a',\n",
       "  3: 'b',\n",
       "  4: 'c',\n",
       "  5: 'd',\n",
       "  6: 'e',\n",
       "  7: 'f',\n",
       "  8: 'g',\n",
       "  9: 'h',\n",
       "  10: 'i',\n",
       "  11: 'j',\n",
       "  12: 'k',\n",
       "  13: 'l',\n",
       "  14: 'm',\n",
       "  15: 'n',\n",
       "  16: 'o',\n",
       "  17: 'p',\n",
       "  18: 'q',\n",
       "  19: 'r',\n",
       "  20: 's',\n",
       "  21: 't',\n",
       "  22: 'u',\n",
       "  23: 'v',\n",
       "  24: 'w',\n",
       "  25: 'x',\n",
       "  26: 'y',\n",
       "  27: 'z',\n",
       "  28: 'æ',\n",
       "  29: 'é',\n",
       "  30: 'ë',\n",
       "  31: 'ö',\n",
       "  32: 'œ'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_single_language_stem_dataset(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livonian\n",
      "['0', 'a', 'b', 'd', 'e', 'f', 'g', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'z', 'ä', 'õ', 'ā', 'ē', 'ī', 'ļ', 'ņ', 'ō', 'ŗ', 'š', 'ū', 'ž', 'ǟ', 'ț', 'ȭ', 'ȯ', 'ȱ', 'ḑ']\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 19:38:23.645722: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-09 19:38:23.646815: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "languages = [\"livonian\"]\n",
    "\n",
    "for language in languages:\n",
    "    print(language)\n",
    "    X_train, X_test, y_train, y_test, X_vocab, lut = create_single_language_stem_dataset(language, hilo=\"low\")\n",
    "\n",
    "    vocab_len = len(X_vocab)\n",
    "\n",
    "    def distribution_to_sequence(batch_output):\n",
    "        return tf.one_hot(tf.argmax(batch_output, axis=-1), depth = vocab_len)\n",
    "\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        y_true_adjusted = tf.math.subtract(tf.math.multiply(y_true, 2), 1)\n",
    "        y_pred_adjusted = tf.math.subtract(tf.math.multiply(y_pred, 2), 1)\n",
    "        return backend.mean(y_true_adjusted * y_pred_adjusted)\n",
    "\n",
    "    generator = keras.models.Sequential([\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dense(vocab_len, activation=\"softmax\")\n",
    "    ])\n",
    "    discriminator = keras.models.Sequential([\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        # keras.layers.Dropout(0.2),\n",
    "        # keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "    discriminator.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\", run_eagerly=True)\n",
    "    discriminator.trainable = False\n",
    "    gan.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\", run_eagerly=True)\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "    def train_gan(gan, dataset, batch_size, vocab_len, n_epochs=50):\n",
    "        generator, discriminator = gan.layers\n",
    "        discriminator_history = {\"loss\":[],\"accuracy\":[]}\n",
    "        generator_history = {\"generated_sequences\":[]}\n",
    "        gan_history = {\"loss\":[],\"accuracy\":[]}\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "            discriminator_metrics = []\n",
    "            gan_metrics = []\n",
    "            generated_images = None\n",
    "            for X_batch in dataset:\n",
    "                # phase 1 - training the discriminator\n",
    "                noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "                generated_images = generator(noise)\n",
    "                X_fake_and_real = tf.concat([generated_images, tf.cast(X_batch, dtype=\"float32\")], axis=0)\n",
    "                y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "                discriminator.trainable = True\n",
    "                discriminator_metrics = discriminator.train_on_batch(tf.cast(X_fake_and_real, dtype=\"float32\"), y1)\n",
    "                # phase 2 - training the generator\n",
    "                noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "                y2 = tf.constant([[1.]] * batch_size)\n",
    "                discriminator.trainable = False\n",
    "                gan_metrics = gan.train_on_batch(noise, y2)\n",
    "            for i,v in enumerate(discriminator_metrics):\n",
    "                discriminator_history[discriminator.metrics_names[i]].append(v)\n",
    "            for i,v in enumerate(gan_metrics):\n",
    "                gan_history[gan.metrics_names[i]].append(v)\n",
    "            generator_history[\"generated_sequences\"].append(generated_images[0])\n",
    "            history_df = pd.DataFrame({\n",
    "                \"discriminator_loss\":discriminator_history[\"loss\"],\n",
    "                \"discriminator_accuracy\":discriminator_history[\"accuracy\"],\n",
    "                \"gan_loss\":gan_history[\"loss\"],\n",
    "                \"gan_accuracy\":gan_history[\"accuracy\"],\n",
    "                \"generated_sequences\":[\"\".join([lut[np.argmax(one_hot_char)] for one_hot_char in one_hot_sequence]) for one_hot_sequence in generator_history[\"generated_sequences\"]],\n",
    "            })\n",
    "            history_df.to_csv(f'{language}_gan_history.csv')\n",
    "            if epoch % 50 == 0:\n",
    "                gan.save(f'checkpoints/{language}-epoch-{epoch}.h5', save_format=\"h5\")\n",
    "        return {\"discriminator\": discriminator_history, \"generator\": generator_history, \"gan\": gan_history}\n",
    "\n",
    "    history = train_gan(gan, dataset, batch_size, vocab_len, n_epochs=1500)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
