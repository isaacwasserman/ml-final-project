{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import align\n",
    "import argparse\n",
    "import codecs\n",
    "import os, sys\n",
    "from random import random, choice\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from losses_impl import *\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stem_length = 20\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as inp:\n",
    "        lines = inp.readlines()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    tags = []\n",
    "    for l in lines:\n",
    "        l = l.strip().split('\\t')\n",
    "        if l:\n",
    "            inputs.append(list(l[0].strip()))\n",
    "            outputs.append(list(l[1].strip()))\n",
    "            tags.append(re.split('\\W+', l[2].strip()))\n",
    "    return inputs, outputs, tags\n",
    "\n",
    "def find_good_range(a,b):\n",
    "\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tif sum(mask) == 0:\n",
    "\t\t# Some times the alignment is off-by-one\n",
    "\t\tb = [' '] + b\n",
    "\t\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tranges = []\n",
    "\tprev = False\n",
    "\tfor i,k in enumerate(mask):\n",
    "\t\tif k and prev:\n",
    "\t\t\tprev = True\n",
    "\t\telif k and not prev:\n",
    "\t\t\tstart = i\n",
    "\t\t\tprev = True\n",
    "\t\telif prev and not k:\n",
    "\t\t\tend = i\n",
    "\t\t\tranges.append((start, end))\n",
    "\t\t\tprev = False\n",
    "\t\telif not prev and not k:\n",
    "\t\t\tprev = False\n",
    "\tif prev:\n",
    "\t\tranges.append((start,i+1))\n",
    "\tranges = [c for c in ranges if c[1]-c[0]>2]\n",
    "\treturn ranges\n",
    "def generate_stem():\n",
    "\treturn \"___\"\n",
    "\n",
    "def get_chars(l):\n",
    "    flat_list = [char for word in l for char in word]\n",
    "    return list(set(flat_list))\n",
    "def best_range(ranges):\n",
    "    longest_length = 0\n",
    "    longest_index = 0\n",
    "    for i,r in enumerate(ranges):\n",
    "        length = r[1] - r[0]\n",
    "        if length > longest_length:\n",
    "            longest_length = length\n",
    "            longest_index = i\n",
    "    return ranges[i]\n",
    "\n",
    "def augment(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "    aligned = align.Aligner(temp).alignedpairs\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    new_tags = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            new_i, new_o = list(i), list(o)\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            new_stem = generate_stem()\n",
    "            new_i[s:e] = new_stem\n",
    "            new_o[s:e] = new_stem\n",
    "            new_i1 = [c for l,c in enumerate(new_i) if (c.strip() or (new_o[l]==' ' and new_i[l] == ' '))]\n",
    "            new_o1 = [c for l,c in enumerate(new_o) if (c.strip() or (new_i[l]==' ' and new_o[l] == ' '))]\n",
    "            new_inputs.append(new_i1)\n",
    "            new_outputs.append(new_o1)\n",
    "            new_tags.append(tags[k])\n",
    "        else:\n",
    "            new_inputs.append([])\n",
    "            new_outputs.append([])\n",
    "            new_tags.append([])\n",
    "    return new_inputs, new_outputs, new_tags\n",
    "\n",
    "def find_stems(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        aligned = align.Aligner(temp).alignedpairs\n",
    "\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stems = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            stem = o[s:e]\n",
    "            stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def get_vocab(strings):\n",
    "    return sorted(list(get_chars(strings)))\n",
    "\n",
    "def enumerate_sequence_characters(sequences, vocab):\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([lut[char] for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def one_hot_encode_sequence(sequences, vocab):\n",
    "    length = len(vocab)\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([[0] * lut[char] + [1] + [0] * ((length - lut[char]) - 1) for char in sequence])\n",
    "    return np.array(new_sequences), {v: k for k, v in lut.items()}\n",
    "\n",
    "def get_stem_data(language, set_type=\"train\", hilo=None, data_dir=\"sigmorphon_data\", pad=True):\n",
    "    if hilo is None:\n",
    "        if f'{data_dir}/{language}-{set_type}-high' in glob.glob(f'{data_dir}/{language}-{set_type}-*'):\n",
    "            hilo = \"high\"\n",
    "        else:\n",
    "            hilo = \"low\"\n",
    "    dpath = f'{data_dir}/{language}-{set_type}-{hilo}'\n",
    "    padded_stems = sequence.pad_sequences(find_stems(dpath), dtype=str, maxlen=max_stem_length, padding=\"post\", truncating=\"post\")\n",
    "    if pad:\n",
    "        return padded_stems\n",
    "    else:\n",
    "        return np.array([np.array(stem) for stem in find_stems(dpath)])\n",
    "\n",
    "def clean_stems(affirmative_stems, negative_stems):\n",
    "    excluded_chars = [\",\",\"'\",\"/\",\"*\",\"-\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    affirmative_vocab = get_vocab(affirmative_stems)\n",
    "    to_be_excluded = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        for char in stem:\n",
    "            if char not in affirmative_vocab or char in excluded_chars:\n",
    "                to_be_excluded.append(i)\n",
    "                break\n",
    "    new_negative_stems = []\n",
    "    new_affirmative_stems = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        if i not in to_be_excluded:\n",
    "            new_negative_stems.append([character.lower() for character in stem])\n",
    "    for i,stem in enumerate(affirmative_stems):\n",
    "        reject = False\n",
    "        for char in stem:\n",
    "            if char in excluded_chars:\n",
    "                reject = True\n",
    "        if not reject:\n",
    "            new_affirmative_stems.append([character.lower() for character in stem])\n",
    "    return new_affirmative_stems, new_negative_stems\n",
    "\n",
    "def create_stem_dataset(reference_language, other_languages):\n",
    "    reference_stems = get_stem_data(reference_language)\n",
    "    other_stems = np.concatenate([get_stem_data(language) for language in other_languages])\n",
    "    cleaned = clean_stems(reference_stems, other_stems)\n",
    "    reference_stems = cleaned[0]\n",
    "    other_stems = cleaned[1]\n",
    "    combined_stems = np.concatenate([reference_stems, other_stems])\n",
    "    combined_vocab = get_vocab(combined_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(combined_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems) + [0] * len(other_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab\n",
    "\n",
    "def create_single_language_stem_dataset(language):\n",
    "    reference_stems = get_stem_data(language)\n",
    "    cleaned = clean_stems(reference_stems, np.array([]))\n",
    "    reference_stems = cleaned[0]\n",
    "    combined_vocab = get_vocab(reference_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(reference_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab, lut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'é', 'ë', 'ö', 'œ']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_vocab, lut = create_single_language_stem_dataset(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "Epoch 2/2000\n",
      "Epoch 3/2000\n",
      "Epoch 4/2000\n",
      "Epoch 5/2000\n",
      "Epoch 6/2000\n",
      "Epoch 7/2000\n",
      "Epoch 8/2000\n",
      "Epoch 9/2000\n",
      "Epoch 10/2000\n",
      "Epoch 11/2000\n",
      "Epoch 12/2000\n",
      "Epoch 13/2000\n",
      "Epoch 14/2000\n",
      "Epoch 15/2000\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(X_vocab)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)\n",
    "\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.Dense(vocab_len, activation=\"softmax\")\n",
    "])\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "    # keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "discriminator.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=wasserstein_loss, optimizer=\"adam\", metrics=\"accuracy\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "def distribution_to_sequence(batch_output):\n",
    "    return tf.one_hot(tf.argmax(batch_output, axis=-1), depth = vocab_len)\n",
    "\n",
    "def train_gan(gan, dataset, batch_size, vocab_len, n_epochs=50):\n",
    "    generator, discriminator = gan.layers\n",
    "    discriminator_history = {\"loss\":[],\"accuracy\":[]}\n",
    "    generator_history = {\"generated_sequences\":[]}\n",
    "    gan_history = {\"loss\":[],\"accuracy\":[]}\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "        discriminator_metrics = []\n",
    "        gan_metrics = []\n",
    "        generated_images = None\n",
    "        for X_batch in dataset:\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "            # generated_images = distribution_to_sequence(generator(noise))\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, tf.cast(X_batch, dtype=\"float32\")], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.trainable = True\n",
    "            discriminator_metrics = discriminator.train_on_batch(tf.cast(X_fake_and_real, dtype=\"float32\"), y1)\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size,max_stem_length,vocab_len])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan_metrics = gan.train_on_batch(noise, y2)\n",
    "        for i,v in enumerate(discriminator_metrics):\n",
    "            discriminator_history[discriminator.metrics_names[i]].append(v)\n",
    "        for i,v in enumerate(gan_metrics):\n",
    "            gan_history[gan.metrics_names[i]].append(v)\n",
    "        generator_history[\"generated_sequences\"].append(generated_images[0])\n",
    "        history_df = pd.DataFrame({\n",
    "            \"discriminator_loss\":discriminator_history[\"loss\"],\n",
    "            \"discriminator_accuracy\":discriminator_history[\"accuracy\"],\n",
    "            \"gan_loss\":gan_history[\"loss\"],\n",
    "            \"gan_accuracy\":gan_history[\"accuracy\"],\n",
    "            \"generated_sequences\":[\"\".join([lut[np.argmax(one_hot_char)] for one_hot_char in one_hot_sequence]) for one_hot_sequence in generator_history[\"generated_sequences\"]],\n",
    "        })\n",
    "        history_df.to_csv(\"gan_history.csv\")\n",
    "    return {\"discriminator\": discriminator_history, \"generator\": generator_history, \"gan\": gan_history}\n",
    "\n",
    "history = train_gan(gan, dataset, batch_size, vocab_len, n_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
