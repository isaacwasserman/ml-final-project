{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import align\n",
    "import argparse\n",
    "import codecs\n",
    "import os, sys\n",
    "from random import random, choice\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from losses_impl import *\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stem_length = 10\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as inp:\n",
    "        lines = inp.readlines()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    tags = []\n",
    "    for l in lines:\n",
    "        l = l.strip().split('\\t')\n",
    "        if l:\n",
    "            inputs.append(list(l[0].strip()))\n",
    "            outputs.append(list(l[1].strip()))\n",
    "            tags.append(re.split('\\W+', l[2].strip()))\n",
    "    return inputs, outputs, tags\n",
    "\n",
    "def find_good_range(a,b):\n",
    "\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tif sum(mask) == 0:\n",
    "\t\t# Some times the alignment is off-by-one\n",
    "\t\tb = [' '] + b\n",
    "\t\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tranges = []\n",
    "\tprev = False\n",
    "\tfor i,k in enumerate(mask):\n",
    "\t\tif k and prev:\n",
    "\t\t\tprev = True\n",
    "\t\telif k and not prev:\n",
    "\t\t\tstart = i\n",
    "\t\t\tprev = True\n",
    "\t\telif prev and not k:\n",
    "\t\t\tend = i\n",
    "\t\t\tranges.append((start, end))\n",
    "\t\t\tprev = False\n",
    "\t\telif not prev and not k:\n",
    "\t\t\tprev = False\n",
    "\tif prev:\n",
    "\t\tranges.append((start,i+1))\n",
    "\tranges = [c for c in ranges if c[1]-c[0]>2]\n",
    "\treturn ranges\n",
    "def generate_stem():\n",
    "\treturn \"___\"\n",
    "\n",
    "def get_chars(l):\n",
    "    flat_list = [char for word in l for char in word]\n",
    "    return list(set(flat_list))\n",
    "def best_range(ranges):\n",
    "    longest_length = 0\n",
    "    longest_index = 0\n",
    "    for i,r in enumerate(ranges):\n",
    "        length = r[1] - r[0]\n",
    "        if length > longest_length:\n",
    "            longest_length = length\n",
    "            longest_index = i\n",
    "    return ranges[i]\n",
    "\n",
    "def augment(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "    aligned = align.Aligner(temp).alignedpairs\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    new_tags = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            new_i, new_o = list(i), list(o)\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            new_stem = generate_stem()\n",
    "            new_i[s:e] = new_stem\n",
    "            new_o[s:e] = new_stem\n",
    "            new_i1 = [c for l,c in enumerate(new_i) if (c.strip() or (new_o[l]==' ' and new_i[l] == ' '))]\n",
    "            new_o1 = [c for l,c in enumerate(new_o) if (c.strip() or (new_i[l]==' ' and new_o[l] == ' '))]\n",
    "            new_inputs.append(new_i1)\n",
    "            new_outputs.append(new_o1)\n",
    "            new_tags.append(tags[k])\n",
    "        else:\n",
    "            new_inputs.append([])\n",
    "            new_outputs.append([])\n",
    "            new_tags.append([])\n",
    "    return new_inputs, new_outputs, new_tags\n",
    "\n",
    "def find_stems(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        aligned = align.Aligner(temp).alignedpairs\n",
    "\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stems = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            stem = o[s:e]\n",
    "            stems.append(stem)\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    return stems\n",
    "\n",
    "def get_vocab(strings):\n",
    "    return sorted(list(get_chars(strings)))\n",
    "\n",
    "def enumerate_sequence_characters(sequences, vocab):\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([lut[char] for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def one_hot_encode_sequence(sequences, vocab):\n",
    "    length = len(vocab)\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([[0] * lut[char] + [1] + [0] * ((length - lut[char]) - 1) for char in sequence])\n",
    "    return np.array(new_sequences), {v: k for k, v in lut.items()}\n",
    "\n",
    "def get_stem_data(language, set_type=\"train\", hilo=None, data_dir=\"sigmorphon_data\", pad=True):\n",
    "    if hilo is None:\n",
    "        if f'{data_dir}/{language}-{set_type}-high' in glob.glob(f'{data_dir}/{language}-{set_type}-*'):\n",
    "            hilo = \"high\"\n",
    "        else:\n",
    "            hilo = \"low\"\n",
    "    dpath = f'{data_dir}/{language}-{set_type}-{hilo}'\n",
    "    padded_stems = sequence.pad_sequences(find_stems(dpath), dtype=str, maxlen=max_stem_length, padding=\"post\", truncating=\"post\")\n",
    "    if pad:\n",
    "        return padded_stems\n",
    "    else:\n",
    "        return np.array([np.array(stem) for stem in find_stems(dpath)])\n",
    "\n",
    "def clean_stems(affirmative_stems, negative_stems):\n",
    "    excluded_chars = [\",\",\"'\",\"/\",\"*\",\"-\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    affirmative_vocab = get_vocab(affirmative_stems)\n",
    "    to_be_excluded = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        for char in stem:\n",
    "            if char not in affirmative_vocab or char in excluded_chars:\n",
    "                to_be_excluded.append(i)\n",
    "                break\n",
    "    new_negative_stems = []\n",
    "    new_affirmative_stems = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        if i not in to_be_excluded:\n",
    "            new_negative_stems.append([character.lower() for character in stem])\n",
    "    for i,stem in enumerate(affirmative_stems):\n",
    "        reject = False\n",
    "        for char in stem:\n",
    "            if char in excluded_chars:\n",
    "                reject = True\n",
    "        if not reject:\n",
    "            new_affirmative_stems.append([character.lower() for character in stem])\n",
    "    return new_affirmative_stems, new_negative_stems\n",
    "\n",
    "def create_stem_dataset(reference_language, other_languages):\n",
    "    reference_stems = get_stem_data(reference_language)\n",
    "    other_stems = np.concatenate([get_stem_data(language) for language in other_languages])\n",
    "    cleaned = clean_stems(reference_stems, other_stems)\n",
    "    reference_stems = cleaned[0]\n",
    "    other_stems = cleaned[1]\n",
    "    combined_stems = np.concatenate([reference_stems, other_stems])\n",
    "    combined_vocab = get_vocab(combined_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(combined_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems) + [0] * len(other_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab\n",
    "\n",
    "def create_single_language_stem_dataset(language, hilo=\"high\"):\n",
    "    reference_stems = get_stem_data(language, set_type=\"train\", hilo=hilo)\n",
    "    cleaned = clean_stems(reference_stems, np.array([]))\n",
    "    reference_stems = cleaned[0]\n",
    "    combined_vocab = get_vocab(reference_stems)\n",
    "    print(combined_vocab)\n",
    "    X,lut = one_hot_encode_sequence(reference_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.001)\n",
    "    return [\"\".join(stem).replace(\"0\",\"\") for stem in reference_stems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '0', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'û', 'ü', 'œ']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [path.replace(\"sigmorphon_data/\",\"\").replace(\"-train-low\",\"\") for path in glob.glob(\"sigmorphon_data/*-train-low\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    X = create_single_language_stem_dataset(language, hilo=\"high\")\n",
    "    n = 3\n",
    "    model = nltk.lm.MLE(n)\n",
    "    train, vocab = padded_everygram_pipeline(n, X)\n",
    "    model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ter'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = model.generate(10)\n",
    "while \"\".join([char for char in gen if char != \"<s>\" and char != \"</s>\"]) == \"\":\n",
    "    gen = model.generate(10)\n",
    "\"\".join([char for char in gen if char != \"<s>\" and char != \"</s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
