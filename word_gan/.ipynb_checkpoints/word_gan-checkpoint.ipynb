{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import align\n",
    "import argparse\n",
    "import codecs\n",
    "import os, sys\n",
    "from random import random, choice\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as inp:\n",
    "        lines = inp.readlines()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    tags = []\n",
    "    for l in lines:\n",
    "        l = l.strip().split('\\t')\n",
    "        if l:\n",
    "            inputs.append(list(l[0].strip()))\n",
    "            outputs.append(list(l[1].strip()))\n",
    "            tags.append(re.split('\\W+', l[2].strip()))\n",
    "    return inputs, outputs, tags\n",
    "\n",
    "def find_good_range(a,b):\n",
    "\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tif sum(mask) == 0:\n",
    "\t\t# Some times the alignment is off-by-one\n",
    "\t\tb = [' '] + b\n",
    "\t\tmask = [(a[i]==b[i] and a[i] != u\" \") for i in range(len(a))]\n",
    "\tranges = []\n",
    "\tprev = False\n",
    "\tfor i,k in enumerate(mask):\n",
    "\t\tif k and prev:\n",
    "\t\t\tprev = True\n",
    "\t\telif k and not prev:\n",
    "\t\t\tstart = i\n",
    "\t\t\tprev = True\n",
    "\t\telif prev and not k:\n",
    "\t\t\tend = i\n",
    "\t\t\tranges.append((start, end))\n",
    "\t\t\tprev = False\n",
    "\t\telif not prev and not k:\n",
    "\t\t\tprev = False\n",
    "\tif prev:\n",
    "\t\tranges.append((start,i+1))\n",
    "\tranges = [c for c in ranges if c[1]-c[0]>2]\n",
    "\treturn ranges\n",
    "def generate_stem():\n",
    "\treturn \"___\"\n",
    "\n",
    "def get_chars(l):\n",
    "    flat_list = [char for word in l for char in word]\n",
    "    return list(set(flat_list))\n",
    "def best_range(ranges):\n",
    "    longest_length = 0\n",
    "    longest_index = 0\n",
    "    for i,r in enumerate(ranges):\n",
    "        length = r[1] - r[0]\n",
    "        if length > longest_length:\n",
    "            longest_length = length\n",
    "            longest_index = i\n",
    "    return ranges[i]\n",
    "\n",
    "def augment(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "    aligned = align.Aligner(temp).alignedpairs\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_inputs = []\n",
    "    new_outputs = []\n",
    "    new_tags = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            new_i, new_o = list(i), list(o)\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            new_stem = generate_stem()\n",
    "            new_i[s:e] = new_stem\n",
    "            new_o[s:e] = new_stem\n",
    "            new_i1 = [c for l,c in enumerate(new_i) if (c.strip() or (new_o[l]==' ' and new_i[l] == ' '))]\n",
    "            new_o1 = [c for l,c in enumerate(new_o) if (c.strip() or (new_i[l]==' ' and new_o[l] == ' '))]\n",
    "            new_inputs.append(new_i1)\n",
    "            new_outputs.append(new_o1)\n",
    "            new_tags.append(tags[k])\n",
    "        else:\n",
    "            new_inputs.append([])\n",
    "            new_outputs.append([])\n",
    "            new_tags.append([])\n",
    "    return new_inputs, new_outputs, new_tags\n",
    "\n",
    "def find_stems(input_path):\n",
    "    inputs,outputs,tags = np.array(read_data(input_path), dtype=object)\n",
    "    temp = [(''.join(inputs[i]), ''.join(outputs[i])) for i in range(len(outputs))]\n",
    "\n",
    "    with io.capture_output() as captured:\n",
    "        aligned = align.Aligner(temp).alignedpairs\n",
    "\n",
    "    vocab = list(get_chars(inputs + outputs))\n",
    "    try:\n",
    "        vocab.remove(u\" \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stems = []\n",
    "    for k,item in enumerate(aligned):\n",
    "        i,o = item[0],item[1]\n",
    "        good_range = find_good_range(i, o)\n",
    "        if good_range:\n",
    "            r = best_range(good_range)\n",
    "            s = r[0]\n",
    "            e = r[1]\n",
    "            if (e-s>5): #arbitrary value\n",
    "                s += 1\n",
    "                e -= 1\n",
    "            stem = o[s:e]\n",
    "            stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def get_vocab(strings):\n",
    "    return sorted(list(get_chars(strings)))\n",
    "\n",
    "def enumerate_sequence_characters(sequences, vocab):\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([lut[char] for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def one_hot_encode_sequence(sequences, vocab):\n",
    "    length = len(vocab)\n",
    "    lut = {\"0\":0}\n",
    "    count = 1\n",
    "    for character in vocab:\n",
    "        if character != \"0\":\n",
    "            lut[character] = count\n",
    "            count += 1\n",
    "    new_sequences = []\n",
    "    for sequence in sequences:\n",
    "        new_sequences.append([[0] * lut[char] + [1] + [0] * ((length - lut[char]) - 1) for char in sequence])\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def get_stem_data(language, set_type=\"train\", hilo=None, data_dir=\"sigmorphon_data\"):\n",
    "    if hilo is None:\n",
    "        if f'{data_dir}/{language}-{set_type}-high' in glob.glob(f'{data_dir}/{language}-{set_type}-*'):\n",
    "            hilo = \"high\"\n",
    "        else:\n",
    "            hilo = \"low\"\n",
    "    dpath = f'{data_dir}/{language}-{set_type}-{hilo}'\n",
    "    max_stem_length = 20\n",
    "    padded_stems = sequence.pad_sequences(find_stems(dpath), dtype=str, maxlen=max_stem_length, padding=\"post\", truncating=\"post\")\n",
    "    return padded_stems\n",
    "\n",
    "def clean_stems(affirmative_stems, negative_stems):\n",
    "    excluded_chars = [\",\",\"'\",\"/\",\"*\",\"-\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    affirmative_vocab = get_vocab(affirmative_stems)\n",
    "    to_be_excluded = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        for char in stem:\n",
    "            print(affirmative_vocab)\n",
    "            if char not in affirmative_vocab or char in excluded_chars:\n",
    "                to_be_excluded.append(i)\n",
    "                break\n",
    "    new_negative_stems = []\n",
    "    new_affirmative_stems = []\n",
    "    for i,stem in enumerate(negative_stems):\n",
    "        if i not in to_be_excluded:\n",
    "            new_negative_stems.append([character.lower() for character in stem])\n",
    "    for i,stem in enumerate(affirmative_stems):\n",
    "        reject = False\n",
    "        for char in stem:\n",
    "            if char in excluded_chars:\n",
    "                reject = True\n",
    "        if not reject:\n",
    "            new_affirmative_stems.append([character.lower() for character in stem])\n",
    "    return new_affirmative_stems, new_negative_stems\n",
    "\n",
    "def create_stem_dataset(reference_language, other_languages):\n",
    "    reference_stems = get_stem_data(reference_language)\n",
    "    other_stems = np.concatenate([get_stem_data(language) for language in other_languages])\n",
    "    cleaned = clean_stems(reference_stems, other_stems)\n",
    "    reference_stems = cleaned[0]\n",
    "    other_stems = cleaned[1]\n",
    "    combined_stems = np.concatenate([reference_stems, other_stems])\n",
    "    combined_vocab = get_vocab(combined_stems)\n",
    "    print(combined_vocab)\n",
    "    X = one_hot_encode_sequence(combined_stems, combined_vocab)\n",
    "    labels = np.array([1] * len(reference_stems) + [0] * len(other_stems))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test, combined_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_vocab = create_stem_dataset(\"english\", [\"basque\",\"french\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemGenerator(tf.keras.Model):\n",
    "  def __init__(self, vocab):\n",
    "    super().__init__(self)\n",
    "    self.vocab = vocab\n",
    "    vocab_len = len(vocab)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_len, 8)\n",
    "    self.gru = tf.keras.layers.GRU(100,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_len)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "def StemDiscriminator():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(65))\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "generator = StemGenerator(X_vocab)\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated = generator(noise, training=False)\n",
    "\n",
    "print(X_vocab[np.argmax(generated[0,0,:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'é'\",\n",
       " \"'o'\",\n",
       " \"['y'\",\n",
       " \"'a'\",\n",
       " \"'u'\",\n",
       " \"'x'\",\n",
       " \"'m'\",\n",
       " 'G',\n",
       " \"['b'\",\n",
       " \"'w'\",\n",
       " '/',\n",
       " \"['x'\",\n",
       " \"['v'\",\n",
       " \"['g'\",\n",
       " 'r',\n",
       " 't',\n",
       " \"['é'\",\n",
       " '*',\n",
       " 'i',\n",
       " \"'t'\",\n",
       " 'L',\n",
       " 'o',\n",
       " 'U',\n",
       " \"['d'\",\n",
       " 'w',\n",
       " 'M',\n",
       " \"['q'\",\n",
       " \"'d'\",\n",
       " 's',\n",
       " \"['p'\",\n",
       " \"['æ'\",\n",
       " \"['a'\",\n",
       " 'm',\n",
       " \"'ë'\",\n",
       " 'u',\n",
       " \"'h'\",\n",
       " 'e',\n",
       " \"['h'\",\n",
       " \"['n'\",\n",
       " 'x',\n",
       " 'l',\n",
       " 'j',\n",
       " \"'k'\",\n",
       " 'ë',\n",
       " 'z',\n",
       " 'ö',\n",
       " \"['k'\",\n",
       " \"['m'\",\n",
       " '0',\n",
       " \"['f'\",\n",
       " 'y',\n",
       " \"'s'\",\n",
       " \"['l'\",\n",
       " \"'v'\",\n",
       " \"'æ'\",\n",
       " 'd',\n",
       " 'n',\n",
       " \"['o'\",\n",
       " 'q',\n",
       " \"'\",\n",
       " \"'r'\",\n",
       " \"['w'\",\n",
       " \"'0']\",\n",
       " \"'g'\",\n",
       " 'v',\n",
       " \"'q'\",\n",
       " 'œ',\n",
       " \"'j'\",\n",
       " 'f',\n",
       " \"['-'\",\n",
       " \"'0'\",\n",
       " \"['z'\",\n",
       " \"'b'\",\n",
       " 'D',\n",
       " 'B',\n",
       " 'a',\n",
       " \"'œ'\",\n",
       " 'c',\n",
       " 'g',\n",
       " 'é',\n",
       " '1',\n",
       " \"['i'\",\n",
       " 'k',\n",
       " \"['j'\",\n",
       " '[\"\\'\"',\n",
       " 'b',\n",
       " \"['c'\",\n",
       " 'P',\n",
       " \"'e'\",\n",
       " 'R',\n",
       " \"'i'\",\n",
       " 'S',\n",
       " \"['s'\",\n",
       " 'I',\n",
       " \"'n'\",\n",
       " 'V',\n",
       " \"'y'\",\n",
       " \"'l'\",\n",
       " 'F',\n",
       " \"'f'\",\n",
       " 'C',\n",
       " \"['r'\",\n",
       " '-',\n",
       " \"['u'\",\n",
       " 'Q',\n",
       " 'A',\n",
       " 'X',\n",
       " \"['t'\",\n",
       " 'O',\n",
       " \"'p'\",\n",
       " 'æ',\n",
       " \"'-'\",\n",
       " '9',\n",
       " 'p',\n",
       " \"'c'\",\n",
       " \"['e'\",\n",
       " \"'z'\",\n",
       " 'h']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9df139cf4fd7fbfed7596b00795d916641fdf384a73a205999ad45fcffc5436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
